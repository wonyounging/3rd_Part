{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 문장 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-Neo : GPT의 오픈소스 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers sentencepiece\n",
    "\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "#           형태소 분석, 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   40, 16726,   262,  2854,   286,   402, 11571,    12,  8199,    78,\n",
      "         4166,   416,  4946, 20185,    13])\n",
      "I evaluated the performance of GPT-Neo developed by OpenAI.\n"
     ]
    }
   ],
   "source": [
    "# 텍스트=>정수 인코딩(1개의 문장)\n",
    "\n",
    "input = tokenizer.encode(\"I evaluated the performance of GPT-Neo developed by OpenAI.\",\n",
    "                         return_tensors=\"pt\")\n",
    "#                                        pt : pytoch / ts : tensor\n",
    "\n",
    "print(input[0])\n",
    "print(tokenizer.decode(input[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   40, 16726,   262,  2854,   286,   402, 11571,    12,  8199,    78,\n",
      "          4166,   416,  4946, 20185,    13],\n",
      "        [   40, 16726,   262,  2854,   286,   402, 11571,  4166,   416,  4946,\n",
      "         20185,    13, 50257, 50257, 50257]])\n",
      "['I evaluated the performance of GPT-Neo developed by OpenAI.', 'I evaluated the performance of GPT developed by OpenAI. [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩(복수 문장)\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#                                           제로패딩\n",
    "\n",
    "input = tokenizer.batch_encode_plus(\n",
    "    [\"I evaluated the performance of GPT-Neo developed by OpenAI.\",\n",
    "     \"I evaluated the performance of GPT developed by OpenAI.\"]\n",
    "     , padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(input['input_ids'])\n",
    "print([tokenizer.decode(input['input_ids'][i]) for i in range(len(input['input_ids']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복수의 문장 인코딩\n",
    "\n",
    "input = tokenizer.batch_encode_plus(\n",
    "\n",
    "    [\"I evaluated the performance of GPT2 developed by OpenAI.\",\n",
    "     \"Vaccine for new coronavirus in the UK\",\n",
    "    \"3.1415926535\"]\n",
    "    , max_length=5, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40, 16726,   262,  2854,   286],\n",
       "        [   53,  4134,   500,   329,   649],\n",
       "        [   18,    13,  1415, 19707, 22980]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코딩 결과 확인\n",
    "\n",
    "input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Python3.9.13\\lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 문장 만들기\n",
    "\n",
    "generated = model.generate(input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.1\n",
      "I evaluated the performance of the proposed method on the real-world dataset. The results are shown in\n",
      "\n",
      "No.2\n",
      "Vaccine for new-borns\n",
      "\n",
      "The vaccine for new-borns is a vaccine\n",
      "\n",
      "No.3\n",
      "3.1415926535897932384626433832795028841971693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 생성된 문장 디코딩\n",
    "\n",
    "generated_text = tokenizer.batch_decode(generated)\n",
    "\n",
    "for i, sentence in enumerate(generated_text):\n",
    "  print(f'No.{i+1}')\n",
    "  print(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 762/762 [00:00<00:00, 71.6kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 5.40MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.29MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 42.6MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 353M/353M [00:03<00:00, 113MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 41.4kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DistilGPT2 모형 활용\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good thing to have\n"
     ]
    }
   ],
   "source": [
    "# DistilGPT2 모형으로 문장 만들기\n",
    "\n",
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, max_length=12)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 8.60kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 641/641 [00:00<00:00, 214kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 5.47MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 40.9MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 351M/351M [00:03<00:00, 106MB/s]  \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 11.4kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DialoGPT 모형 활용\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good way to get a feel for the game.\n"
     ]
    }
   ],
   "source": [
    "#문장 만들기\n",
    "\n",
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, max_length=30)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
    "\n",
    "#좀더 자연스러운 문장이 만들어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Covid19 delta is spreading the word\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Covid19 delta is spreading\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, max_length=10)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 190kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 440M/440M [00:04<00:00, 104MB/s]  \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 2.75kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 34.7MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 885kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1829017698764801,\n",
       "  'token': 2293,\n",
       "  'token_str': 'love',\n",
       "  'sequence': 'i love apple.'},\n",
       " {'score': 0.12623995542526245,\n",
       "  'token': 2066,\n",
       "  'token_str': 'like',\n",
       "  'sequence': 'i like apple.'},\n",
       " {'score': 0.11780297756195068,\n",
       "  'token': 2359,\n",
       "  'token_str': 'wanted',\n",
       "  'sequence': 'i wanted apple.'},\n",
       " {'score': 0.0684230700135231,\n",
       "  'token': 2215,\n",
       "  'token_str': 'want',\n",
       "  'sequence': 'i want apple.'},\n",
       " {'score': 0.05560746043920517,\n",
       "  'token': 3866,\n",
       "  'token_str': 'loved',\n",
       "  'sequence': 'i loved apple.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLM(Masked Language Model) : 문서의 일부를 가리고(mask) 원래 단어를 추측하는 빈칸 채우기\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "#                                                 대소문자 구분 X\n",
    "\n",
    "# [MASK]가 포함된 문장 입력\n",
    "\n",
    "unmasker(\"I [MASK] apple.\")\n",
    "\n",
    "# [MASK] 위치에 입력될 수 있는 단어들을 추천하여 문장 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.06269748508930206,\n",
       "  'token': 8823,\n",
       "  'token_str': 'ate',\n",
       "  'sequence': 'i ate apple.'},\n",
       " {'score': 0.0586443729698658,\n",
       "  'token': 2293,\n",
       "  'token_str': 'love',\n",
       "  'sequence': 'i love apple.'},\n",
       " {'score': 0.056702181696891785,\n",
       "  'token': 3866,\n",
       "  'token_str': 'loved',\n",
       "  'sequence': 'i loved apple.'},\n",
       " {'score': 0.051368582993745804,\n",
       "  'token': 6283,\n",
       "  'token_str': 'hated',\n",
       "  'sequence': 'i hated apple.'},\n",
       " {'score': 0.04913158714771271,\n",
       "  'token': 4521,\n",
       "  'token_str': 'eat',\n",
       "  'sequence': 'i eat apple.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# distilbert 모델\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "unmasker(\"I [MASK] apple.\")\n",
    "\n",
    "# 모형에 따라 추천되는 단어가 달라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 684/684 [00:00<00:00, 68.6kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 47.4M/47.4M [00:00<00:00, 112MB/s]\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 760k/760k [00:00<00:00, 54.2MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.31M/1.31M [00:00<00:00, 6.29MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1261998564004898,\n",
       "  'token': 339,\n",
       "  'token_str': 'love',\n",
       "  'sequence': 'i love apple.'},\n",
       " {'score': 0.092204749584198,\n",
       "  'token': 3345,\n",
       "  'token_str': 'liked',\n",
       "  'sequence': 'i liked apple.'},\n",
       " {'score': 0.056264329701662064,\n",
       "  'token': 2199,\n",
       "  'token_str': 'loved',\n",
       "  'sequence': 'i loved apple.'},\n",
       " {'score': 0.04441859945654869,\n",
       "  'token': 5285,\n",
       "  'token_str': 'hated',\n",
       "  'sequence': 'i hated apple.'},\n",
       " {'score': 0.03994071111083031,\n",
       "  'token': 3223,\n",
       "  'token_str': 'hate',\n",
       "  'sequence': 'i hate apple.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# albert 모델 : bert의 정확도를 높이고 경량화한 모형\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='albert-base-v2')\n",
    "unmasker(\"I [MASK] apple.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
