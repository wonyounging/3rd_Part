{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) MobileBERT / DistilGPT2 / DialoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobile BERT : BERT를 압축하고 속도를 개선한 모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileBertModel(\n",
       "  (embeddings): MobileBertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 512)\n",
       "    (token_type_embeddings): Embedding(2, 512)\n",
       "    (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n",
       "    (LayerNorm): NoNorm()\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): MobileBertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x MobileBertLayer(\n",
       "        (attention): MobileBertAttention(\n",
       "          (self): MobileBertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): MobileBertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): NoNorm()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): MobileBertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): MobileBertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): NoNorm()\n",
       "          (bottleneck): OutputBottleneck(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (LayerNorm): NoNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (bottleneck): Bottleneck(\n",
       "          (input): BottleneckLayer(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): NoNorm()\n",
       "          )\n",
       "          (attention): BottleneckLayer(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): NoNorm()\n",
       "          )\n",
       "        )\n",
       "        (ffn): ModuleList(\n",
       "          (0-2): 3 x FFNLayer(\n",
       "            (intermediate): MobileBertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (intermediate_act_fn): ReLU()\n",
       "            )\n",
       "            (output): FFNOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): NoNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): MobileBertPooler()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MobileBertTokenizer, MobileBertModel\n",
    "import torch\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "tokenizer_mbert = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "\n",
    "model_mbert = MobileBertModel.from_pretrained('google/mobilebert-uncased')\n",
    "model_mbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "model_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mobile', 'bert', 'is', 'more', 'practical', 'than', 'bert', '.']\n",
      "['mobile', 'bert', 'is', 'more', 'practical', 'than', 'bert', '.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "text = \"Mobile bert is more practical than bert.\"\n",
    "\n",
    "# Mobile BERT 토크나이징\n",
    "inputs = tokenizer_mbert.tokenize(text)\n",
    "print(inputs)\n",
    "\n",
    "# BERT 토크나이징\n",
    "inputs = tokenizer_bert.tokenize(text)\n",
    "print(inputs)\n",
    "\n",
    "# 두 모델의 실행 결과가 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "text = \"Mobile bert is more practical than bert.\"\n",
    "\n",
    "inputs = tokenizer_mbert.encode(text)\n",
    "#                          단어 => 숫자\n",
    "# squeeze() 사이즈가 1인 차원 제거, unsqueeze() 첫번째 위치에 1인 차원 추가\n",
    "outputs = model_mbert(torch.tensor(inputs).unsqueeze(0))\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "inputs = tokenizer_bert.encode(text)\n",
    "outputs = model_bert(torch.tensor(inputs).unsqueeze(0))\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# torch.Size([1, 10, 512])  BERT 모형\n",
    "# torch.Size([1, 10, 768])  Mobile BERT 모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the capital of korea is seoul .\n"
     ]
    }
   ],
   "source": [
    "# Mobile BERT 추론\n",
    "\n",
    "from transformers import MobileBertTokenizer, MobileBertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "model = MobileBertForMaskedLM.from_pretrained('google/mobilebert-uncased')\n",
    "\n",
    "# 마스크한 문장 및 정답 문장을 각기 토크나이징\n",
    "inputs = tokenizer(\"The capital of Korea is [MASK].\", return_tensors=\"pt\")\n",
    "labels = tokenizer(\"The capital of Korea is Seoul.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(' '.join([tokenizer.decode(i.item()).replace(\" \", \"\") for i in logits.argmax(-1)[0]][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the capital of korea is seoul .\n"
     ]
    }
   ],
   "source": [
    "# BERT 추론\n",
    "# Mobile BERT 때와 비교하여 토크나이저와 모델이 다름\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(\"The capital of Korea is [MASK].\", return_tensors=\"pt\")\n",
    "labels = tokenizer(\"The capital of Korea is Seoul.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(' '.join([tokenizer.decode(i.item()).replace(\" \", \"\") for i in logits.argmax(-1)[0]][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.9.13\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1468: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good thing to have\n"
     ]
    }
   ],
   "source": [
    "# 문장 완성\n",
    "\n",
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, max_length=12)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good way to get a feel for the game.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, max_length=30)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
