{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) 문장 완성 / 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigBird 모형 : 더 긴 입력 시퀀스를 처리할 수 있는 모형\n",
    "\n",
    "# 2048 토큰 처리(BERT의 4배, 512x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 완성 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading spiece.model: 100%|██████████| 846k/846k [00:00<00:00, 45.7MB/s]\n",
      "c:\\Python3.9.13\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tjoeun\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 775/775 [00:00<00:00, 195kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.02k/1.02k [00:00<00:00, 332kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 760/760 [00:00<00:00, 59.7kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 513M/513M [00:04<00:00, 113MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BigBirdForMaskedLM(\n",
       "  (bert): BigBirdModel(\n",
       "    (embeddings): BigBirdEmbeddings(\n",
       "      (word_embeddings): Embedding(50358, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(4096, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BigBirdEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (cls): BigBirdOnlyMLMHead(\n",
       "    (predictions): BigBirdLMPredictionHead(\n",
       "      (transform): BigBirdPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): NewGELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=50358, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install transformers\n",
    "#pip install sentencepiece\n",
    "\n",
    "from transformers import BigBirdTokenizer, BigBirdForMaskedLM\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "#               토크나이저 - 형태소 분석, 정수 인코딩\n",
    "model = BigBirdForMaskedLM.from_pretrained('google/bigbird-roberta-base')\n",
    "#           모델\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"I like reading [MASK].\", \"I like driving a [MASK].\",\"The world is facing with a [MASK] [MASK] crisis. We are all suffering from infectious diseases.\"]\n",
    "answers = [\"I like reading book.\", \"I like driving a car.\", \"The world is facing with a pandemic crisis. We are all suffering from infectious diseases.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = []\n",
    "encoded_labels =  []\n",
    "\n",
    "for i, l in zip(inputs, answers):\n",
    "#   인덱스, 값\n",
    "  encoded_inputs.append(tokenizer(i, return_tensors=\"pt\"))\n",
    "#                                 입력               pytorch\n",
    "  encoded_labels.append(tokenizer(l, return_tensors=\"pt\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 7 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss：11.183554649353027\n",
      "prediction：i like reading it . i\n",
      "answer：I like reading book.\n",
      "\n",
      "\n",
      "loss：8.30691146850586\n",
      "prediction：its like driving a car . a\n",
      "answer：I like driving a car.\n",
      "\n",
      "\n",
      "loss：4.29605770111084\n",
      "prediction：the world is facing with a global health crisis . we are all suffering from infectious diseases . .\n",
      "answer：The world is facing with a pandemic crisis. We are all suffering from infectious diseases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#추론 모드로 실행\n",
    "\n",
    "for input, label in zip(encoded_inputs, encoded_labels):\n",
    "  outputs = model(**input, labels=label)\n",
    "  loss = outputs.loss\n",
    "  logits = outputs.logits\n",
    "#               final output\n",
    "\n",
    "  print(f\"loss：{loss.item()}\")\n",
    "  print(f\"prediction：{' '.join([tokenizer.decode(logits[0][i].argmax(-1)) for i in range(1, len(logits[0]))])}\")\n",
    "  print(f\"answer：{tokenizer.decode(label[0][1:-1])}\")\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 요약 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 1.91M/1.91M [00:00<00:00, 1.95MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 65.0/65.0 [00:00<00:00, 9.94kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 87.0/87.0 [00:00<00:00, 29.1kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 145kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 2.28G/2.28G [00:26<00:00, 86.1MB/s]\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)neration_config.json: 100%|██████████| 259/259 [00:00<00:00, 23.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Pegasus : 문장 요약에 특화된 사전 학습 모형, 구글 2020 발표\n",
    "\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "\n",
    "# 모델명\n",
    "model_name = 'google/pegasus-xsum'\n",
    "device = 'cpu'\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "          \"\"\"\n",
    "          Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. Recent work shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models.\n",
    "          \"\"\"\n",
    "          ]\n",
    "\n",
    "batch = tokenizer(inputs, truncation=True, padding='longest', return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining large neural language models can lead to substantial gains over continual pretraining of general-domain language models.\n"
     ]
    }
   ],
   "source": [
    "# 요약 문장 생성\n",
    "\n",
    "translated = model.generate(**batch)\n",
    "\n",
    "generated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
