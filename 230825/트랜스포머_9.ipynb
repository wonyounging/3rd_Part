{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) 언어번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 908/908 [00:00<00:00, 279kB/s]\n",
      "c:\\Python3.9.13\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tjoeun\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.94G/1.94G [00:18<00:00, 107MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 233/233 [00:00<00:00, 23.8kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 3.71M/3.71M [00:01<00:00, 3.14MB/s]\n",
      "Downloading (…)tencepiece.bpe.model: 100%|██████████| 2.42M/2.42M [00:00<00:00, 108MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 272/272 [00:00<00:00, 45.5kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 1.14k/1.14k [00:00<00:00, 229kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_text = \"生活就像一盒巧克力。\"\n",
    "\n",
    "tokenizer.src_lang = \"zh\"\n",
    "#                     언어코드(kr, jp, en, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중국어=>영어\n",
    "\n",
    "generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Life is like a box of chocolate.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "#               숫자 => 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Life is like a box of chocolate.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.src_lang = \"ko\"\n",
    "\n",
    "korean_text = \"인생은 한 상자의 초콜릿과 같다.\"\n",
    "\n",
    "encoded_ko = tokenizer(korean_text, return_tensors=\"pt\")\n",
    "\n",
    "generated_tokens = model.generate(**encoded_ko, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! pip install playsound==1.2.2\n",
    "# # ! pip install gtts\n",
    "# from gtts import gTTS\n",
    "# from playsound import playsound\n",
    "\n",
    "# tts = gTTS(text=text[0], lang='en')\n",
    "# tts.save(\"output1.mp3\")     # 텍스트 => 음성파일 생성\n",
    "# playsound(\"output1.mp3\")    # 음성파일 재생\n",
    "# os.remove(\"output1.mp3\")    # 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나는 이 나라가 이 10년이 끝나기 전에 사람을 달에 착륙시키고 그를 안전하게 지구로 돌려보내는 목표를 달성하기 위해 노력해야 한다고 믿는다.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_text = \"I believe this nation should commit itself to achieving the goal, before this decade is out, of landing a man on the moon and returning him safely to the Earth.\"\n",
    "\n",
    "tokenizer.src_lang = \"en\"\n",
    "\n",
    "encoded_en = tokenizer(english_text, return_tensors=\"pt\")\n",
    "\n",
    "generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.get_lang_id(\"ko\"))\n",
    "\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Mobile BERT : BERT를 압축하고 속도를 개선한 모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 19.3MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 847/847 [00:00<00:00, 283kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 147M/147M [00:01<00:00, 106MB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileBertModel(\n",
       "  (embeddings): MobileBertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 512)\n",
       "    (token_type_embeddings): Embedding(2, 512)\n",
       "    (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n",
       "    (LayerNorm): NoNorm()\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): MobileBertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x MobileBertLayer(\n",
       "        (attention): MobileBertAttention(\n",
       "          (self): MobileBertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): MobileBertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): NoNorm()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): MobileBertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): MobileBertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): NoNorm()\n",
       "          (bottleneck): OutputBottleneck(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (LayerNorm): NoNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (bottleneck): Bottleneck(\n",
       "          (input): BottleneckLayer(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): NoNorm()\n",
       "          )\n",
       "          (attention): BottleneckLayer(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): NoNorm()\n",
       "          )\n",
       "        )\n",
       "        (ffn): ModuleList(\n",
       "          (0-2): 3 x FFNLayer(\n",
       "            (intermediate): MobileBertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (intermediate_act_fn): ReLU()\n",
       "            )\n",
       "            (output): FFNOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): NoNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): MobileBertPooler()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MobileBertTokenizer, MobileBertModel\n",
    "import torch\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "tokenizer_mbert = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "#                                                                        대소문자 구분 X\n",
    "model_mbert = MobileBertModel.from_pretrained('google/mobilebert-uncased')\n",
    "model_mbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "model_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mobile', 'bert', 'is', 'more', 'practical', 'than', 'bert', '.']\n",
      "['mobile', 'bert', 'is', 'more', 'practical', 'than', 'bert', '.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "text = \"Mobile bert is more practical than bert.\"\n",
    "\n",
    "# Mobile BERT 토크나이징\n",
    "inputs = tokenizer_mbert.tokenize(text)\n",
    "print(inputs)\n",
    "\n",
    "# BERT 토크나이징\n",
    "inputs = tokenizer_bert.tokenize(text)\n",
    "print(inputs)\n",
    "\n",
    "# 두 모델의 실행 결과가 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "text = \"Mobile bert is more practical than bert.\"\n",
    "\n",
    "inputs = tokenizer_mbert.encode(text)\n",
    "#                        단어 => 숫자\n",
    "# squeeze() 사이즈가 1인 차원 제거, unsqueeze() 첫번째 위치에 1인 차원 추가\n",
    "outputs = model_mbert(torch.tensor(inputs).unsqueeze(0))\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "inputs = tokenizer_bert.encode(text)\n",
    "outputs = model_bert(torch.tensor(inputs).unsqueeze(0))\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# torch.Size([1, 10, 512])  BERT 모형\n",
    "# torch.Size([1, 10, 768])  Mobile BERT 모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the capital of korea is seoul .\n"
     ]
    }
   ],
   "source": [
    "# Mobile BERT 추론\n",
    "\n",
    "from transformers import MobileBertTokenizer, MobileBertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "\n",
    "model = MobileBertForMaskedLM.from_pretrained('google/mobilebert-uncased')\n",
    "\n",
    "# 마스크한 문장 및 정답 문장을 각기 토크나이징\n",
    "inputs = tokenizer(\"The capital of Korea is [MASK].\", return_tensors=\"pt\")\n",
    "\n",
    "labels = tokenizer(\"The capital of Korea is Seoul.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(' '.join([tokenizer.decode(i.item()).replace(\" \", \"\") for i in logits.argmax(-1)[0]][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the capital of korea is seoul .\n"
     ]
    }
   ],
   "source": [
    "# BERT 추론\n",
    "\n",
    "# Mobile BERT 때와 비교하여 토크나이저와 모델이 다름\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"The capital of Korea is [MASK].\", return_tensors=\"pt\")\n",
    "\n",
    "labels = tokenizer(\"The capital of Korea is Seoul.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(' '.join([tokenizer.decode(i.item()).replace(\" \", \"\") for i in logits.argmax(-1)[0]][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.9.13\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1468: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good thing to have\n"
     ]
    }
   ],
   "source": [
    "# 문장 완성\n",
    "\n",
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "\n",
    "greedy_output = model.generate(input_ids, max_length=12)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good way to get a feel for the game.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "\n",
    "greedy_output = model.generate(input_ids, max_length=30)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
