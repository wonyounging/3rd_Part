{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) 앙상블 학습(BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후속문장(이어지는 문장)이면 1, 아니면 0\n",
    "\n",
    "dataset = [[\"What music do you like?\", \"I like Rock music.\", 1],\n",
    "           [\"What is your favorite food?\", \"I like sushi the best\", 1],\n",
    "           [\"What is your favorite color?\", \"I'm going to be a doctor\", 0],\n",
    "           [\"What is your favorite song?\", \"Tokyo olympic game in 2020 was postponed\", 0],\n",
    "           [\"Do you like watching TV shows?\", \"Yeah, I often watch it in my spare time\", 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AdamW\n",
    "from torch import nn\n",
    "\n",
    "#앙상블 학습을 위한 클래스\n",
    "class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n",
    "  def __init__(self, config, *args, **kwargs):\n",
    "      super().__init__(config)\n",
    "\n",
    "      # QA(Question, Answer) BERT 모델\n",
    "      self.bert_model_1 = BertModel(config)\n",
    "\n",
    "      # AQ(Answer, Question) BERT 모델\n",
    "      self.bert_model_2 = BertModel(config)\n",
    "\n",
    "      # 선형함수\n",
    "      self.cls = nn.Linear(2 * self.config.hidden_size, 2)\n",
    "\n",
    "      # 초기 가중치\n",
    "      self.init_weights()\n",
    "\n",
    "  def forward(\n",
    "          self,\n",
    "          input_ids=None,\n",
    "          attention_mask=None,\n",
    "          token_type_ids=None,\n",
    "          position_ids=None,\n",
    "          head_mask=None,\n",
    "          inputs_embeds=None,\n",
    "          next_sentence_label=None,\n",
    "  ):\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    # input_ids 첫번째 입력(문장) 저장\n",
    "    input_ids_1 = input_ids[0]\n",
    "\n",
    "    # input_ids 첫번째 입력(문장)의 attention_mask 저장\n",
    "    attention_mask_1 = attention_mask[0]\n",
    "\n",
    "    # bert_model_1에 input_ids_1 투입한 결과를 outputs에 순차적으로 저장\n",
    "    outputs.append(self.bert_model_1(input_ids_1,\n",
    "                                     attention_mask=attention_mask_1))\n",
    "\n",
    "    # input_ids 두번째 입력(문장) 저장\n",
    "    input_ids_2 = input_ids[1]\n",
    "\n",
    "    # input_ids 두번째 입력(문장)의 attention_mask 저장\n",
    "    attention_mask_2 = attention_mask[1]\n",
    "\n",
    "    # bert_model_2에 input_ids_2 투입한 결과를 outputs에 순차적으로 저장\n",
    "    outputs.append(self.bert_model_2(input_ids_2,\n",
    "                                     attention_mask=attention_mask_2))\n",
    "\n",
    "    # torch.cat()로 텐서 병합\n",
    "    last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
    "\n",
    "    logits = self.cls(last_hidden_states)\n",
    "\n",
    "    if next_sentence_label is not None:\n",
    "      loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "      next_sentence_loss = loss_fct(logits.view(-1, 2), next_sentence_label.view(-1))\n",
    "#                                               2차원                            1차원\n",
    "      return next_sentence_loss, logits\n",
    "    else:\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 로컬에서 실행할 경우 메모리 부족으로 cpu에서 실행\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 모델 및 config 설정\n",
    "config = BertConfig()\n",
    "model = BertEnsembleForNextSentencePrediction(config)\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "learning_rate = 1e-5\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "# 최적화 함수 그룹 파라미터 설정\n",
    "optimizer_grouped_parameters = [{\n",
    "  \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "  }]\n",
    "\n",
    "# 최적화 함수 설정\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 증강 처리 함수\n",
    "# 질문-답변 문장을 답변-질문 순서로 바꿔서 추가\n",
    "\n",
    "def prepare_data(dataset, qa=True):\n",
    "  input_ids, attention_masks = [], []\n",
    "  labels = []\n",
    "  for point in dataset:\n",
    "    if qa is True:\n",
    "      # point에 있는 3개의 원소를 앞에 요소부터 q, a, _ 으로\n",
    "      q, a, _ = point\n",
    "    else:\n",
    "      # point에 있는 3개의 원소를 앞에 요소부터 a, q, _ 으로\n",
    "      a, q, _ = point\n",
    "    # q와 a를 토크나이저를 통해 인코딩\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "      q,  # 문장 1 인코딩\n",
    "      a,  # 문장 2 인코딩\n",
    "      add_special_tokens=True,  # 특수 토큰인 [CLS]와 [SEP] 생성\n",
    "#                                           문장시작 문장구분\n",
    "      max_length=128,          \n",
    "      pad_to_max_length=True,  \n",
    "      return_attention_mask=True,  # attention_mask 생성(패딩 처리된 부분은 1로 표시)\n",
    "      return_tensors='pt',    \n",
    "      truncation=True          \n",
    "    )\n",
    "\n",
    "    input_ids.append(encoded_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "    labels.append(point[-1])\n",
    "\n",
    "  # input_ids를 첫번째 축(dim=0), 즉 세로 방향으로 병합\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "\n",
    "  # attention_mask를 첫번째 축(dim=0), 즉 세로 방향으로 병합\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "  return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler\n",
    "\n",
    "# QADataset 클래스 생성\n",
    "class QADataset(Dataset):\n",
    "  def __init__(self, input_ids, attention_masks, labels=None):\n",
    "    self.input_ids = np.array(input_ids)\n",
    "    self.attention_masks = np.array(attention_masks)\n",
    "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.input_ids[index], self.attention_masks[index], self.labels[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.9.13\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
    "train_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
    "train_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "dataloader_qa =  DataLoader(dataset=train_dataset_qa,\n",
    "                            #batch_size=5,\n",
    "                            batch_size=1,\n",
    "                            sampler=SequentialSampler(train_dataset_qa))\n",
    "\n",
    "dataloader_aq =  DataLoader(dataset=train_dataset_aq,\n",
    "                            #batch_size=5,\n",
    "                            batch_size=1,\n",
    "                            sampler=SequentialSampler(train_dataset_aq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#           gpu 캐시 클리어\n",
    "gc.collect()\n",
    "#      garbage collection 쓰레기 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.5944473743438721\n",
      "epoch:0, loss:0.05763965845108032\n",
      "epoch:0, loss:4.376035690307617\n",
      "epoch:0, loss:3.9338793754577637\n",
      "epoch:0, loss:0.0578121542930603\n",
      "epoch:1, loss:0.11227451264858246\n",
      "epoch:1, loss:0.22196000814437866\n",
      "epoch:1, loss:0.9636520147323608\n",
      "epoch:1, loss:0.48609262704849243\n",
      "epoch:1, loss:1.2660713195800781\n",
      "epoch:2, loss:1.4365160465240479\n",
      "epoch:2, loss:1.4255276918411255\n",
      "epoch:2, loss:0.38666194677352905\n",
      "epoch:2, loss:0.5630022883415222\n",
      "epoch:2, loss:0.8353184461593628\n"
     ]
    }
   ],
   "source": [
    "#앙상블 학습 파인튜닝\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  # dataloader_qa와 datalaode_aq pair를 반복 처리\n",
    "\n",
    "  for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "#                                           질문 => 답변    답변 => 질문\n",
    "#                             인덱스, 데이터\n",
    "    batch_1, batch_2 = combined_batch\n",
    "    model.train()   # 학습모드로 전환\n",
    "    inputs = {\n",
    "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "#                       질답        답질\n",
    "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "        \"next_sentence_label\": batch_1[2]\n",
    "    }\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    print(f\"epoch:{epoch}, loss:{loss}\")\n",
    "    optimizer.step()\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0] [1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 모델 테스트(학습용 데이터)\n",
    "\n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
    "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
    "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "dataloader_qa =  DataLoader(dataset=test_dataset_qa,\n",
    "                            batch_size=16,\n",
    "                            sampler=SequentialSampler(test_dataset_qa))\n",
    "\n",
    "dataloader_aq =  DataLoader(dataset=test_dataset_aq,\n",
    "                            batch_size=16,\n",
    "                            sampler=SequentialSampler(test_dataset_aq))\n",
    "\n",
    "complete_outputs, complete_label_ids = [], []\n",
    "\n",
    "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "  model.eval()\n",
    "  batch_1, batch_2 = combined_batch\n",
    "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    inputs = {\n",
    "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "        \"next_sentence_label\": batch_1[2]\n",
    "    }\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    tmp_eval_loss, logits = outputs[:2]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
    "\n",
    "  complete_outputs.extend(outputs)\n",
    "  complete_label_ids.extend(label_ids)\n",
    "  \n",
    "print(complete_outputs, complete_label_ids)\n",
    "# 예측값과 실제값\n",
    "# 좀더 학습을 진행해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [1]\n"
     ]
    }
   ],
   "source": [
    "# 모델 테스트(새로운 문장)\n",
    "dataset = [[\"What music do you like?\", \"I like Rock music.\", 1]]\n",
    "\n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
    "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
    "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "dataloader_qa =  DataLoader(dataset=test_dataset_qa,\n",
    "                            batch_size=16,\n",
    "                            sampler=SequentialSampler(test_dataset_qa))\n",
    "\n",
    "dataloader_aq =  DataLoader(dataset=test_dataset_aq,\n",
    "                            batch_size=16,\n",
    "                            sampler=SequentialSampler(test_dataset_aq))\n",
    "\n",
    "complete_outputs, complete_label_ids = [], []\n",
    "\n",
    "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "  model.eval()\n",
    "  batch_1, batch_2 = combined_batch\n",
    "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    inputs = {\n",
    "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "        \"next_sentence_label\": batch_1[2]\n",
    "    }\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    tmp_eval_loss, logits = outputs[:2]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
    "\n",
    "  complete_outputs.extend(outputs)\n",
    "  complete_label_ids.extend(label_ids)\n",
    " \n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
    "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
    "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "dataloader_qa =  DataLoader(dataset=test_dataset_qa,\n",
    "                            batch_size=16,\n",
    "                            sampler=SequentialSampler(test_dataset_qa))\n",
    "\n",
    "dataloader_aq =  DataLoader(dataset=test_dataset_aq,\n",
    "                            batch_size=16,\n",
    "                            sampler=SequentialSampler(test_dataset_aq))\n",
    "\n",
    "complete_outputs, complete_label_ids = [], []\n",
    "\n",
    "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "  model.eval()\n",
    "  batch_1, batch_2 = combined_batch\n",
    "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    inputs = {\n",
    "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "        \"next_sentence_label\": batch_1[2]\n",
    "    }\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    tmp_eval_loss, logits = outputs[:2]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
    "\n",
    "  complete_outputs.extend(outputs)\n",
    "  complete_label_ids.extend(label_ids)\n",
    "\n",
    "print(complete_outputs, complete_label_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
