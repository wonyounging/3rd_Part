{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# frozen lake E-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "# Q Table을 모두 0으로 초기화 : 2차원 (number of state, action space) = (16,4)\n",
    "\n",
    "# 현재 state에서 어떤 action을 취할 때 얻을 수 있는 reward를 저장하는 리스트\n",
    "# Q : 주어진 state에서 어떤 action을 취할 것인가에 대한 길잡이\n",
    "\n",
    "# env.observation_space.n : 환경의 경우의 수\n",
    "# env.action_space.n : 행동의 경우의 수\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "print(Q.shape)\n",
    "\n",
    "#하이퍼 파라미터 초기화\n",
    "# 할인율(discount) 정의 => 미래의 보상(reward)을 현재의 보상보다 조금 낮게 계산\n",
    "dis = 0.99\n",
    "\n",
    "# 시도 횟수(에피소드)\n",
    "num_episodes = 1000\n",
    "\n",
    "# 에피소드마다 총 보상의 합을 저장하는 리스트\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q 업데이트 : Q값은 어떤 상태에서 어떤 액션을 취할 때 얻어지는 보상 + action으로 변화된 state에서 얻을 수 있는 reward의 최대값을 더한다.\n",
    "\n",
    "# 즉, 현재의 보상 + 미래에 가능한 보상의 최대값\n",
    "# 할인율: 미래의 보상에 약간의 패널티를 주는 것\n",
    "\n",
    "# Q(state, action) = Reward + max(Q(new state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.9.13\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 방식\n",
    "\n",
    "for i in range(num_episodes) :\n",
    "    state = env.reset(seed=42)[0]\n",
    "    rAll = 0\n",
    "    done = False\n",
    "\n",
    "    while not done :\n",
    "        # Action 중에 가장 R(Reward)이 큰 Action을 랜덤으로 고르는 방식\n",
    "        # env.action_space.n: 4\n",
    "        # randn(1,4) 1행 4열의 정규분포난수\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i+1))\n",
    "\n",
    "        # 해당 Action을 했을 때 environment가 변하고, 새로운 state, reward, done 여부를 반환 받음\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        # Q = R + 할인율*max(Q)\n",
    "        Q[state, action] = reward + dis * np.max(Q[new_state, :])\n",
    "        rAll += reward\n",
    "        state = new_state\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-greedy 방식\n",
    "\n",
    "# 어떠한 확률값 E를 주어 E의 확률로 탐색\n",
    "# E=0.99라면 99%의 확률로 탐색하고 1%의 확률로 개척(새로운 길을 찾음)\n",
    "\n",
    "for i in range(num_episodes) :\n",
    "    state = env.reset(seed=42)[0]\n",
    "    rAll = 0\n",
    "    done = False\n",
    "\n",
    "    # exploration의 확률\n",
    "    e = 1./((i / 100) + 1)\n",
    "\n",
    "    # Q learning 알고리즘\n",
    "    while not done :\n",
    "        # E-Greedy 알고리즘으로 action 고르기\n",
    "        if np.random.rand(1) < e :\n",
    "            action = env.action_space.sample()\n",
    "        else :\n",
    "            action = np.argmax(Q[state, :])\n",
    "\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Q = R + Q\n",
    "        Q[state, action] = reward + dis * np.max(Q[new_state, :])\n",
    "        rAll += reward\n",
    "        state = new_state\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate : 0.178\n",
      "Final Q-Table Values\n",
      "[[0.         0.40473197 0.73970037 0.74717209]\n",
      " [0.         0.         0.         0.74717209]\n",
      " [0.43423133 0.         0.4386175  0.82616862]\n",
      " [0.         0.         0.         0.44304798]\n",
      " [0.40882017 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.90438208 0.         0.89533825 0.        ]\n",
      " [0.93206535 0.87752102 1.         0.90438208]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Success rate : \"+str(sum(rList) / num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
