{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MountainCar Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0') #행동공간연속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 에이전트\n",
    "\n",
    "env.reset()\n",
    "\n",
    "score = 0\n",
    "step = 0\n",
    "\n",
    "for i in range(200):\n",
    "  action = env.action_space.sample()\n",
    "  obs, reward, done, info = env.step(action)[:4]\n",
    "  previous_obs = obs\n",
    "  score += reward\n",
    "  step += 1\n",
    "\n",
    "  if done:\n",
    "    break\n",
    "\n",
    "print(score, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "score = 0\n",
    "step = 0\n",
    "\n",
    "while True:\n",
    "  action = env.action_space.sample()\n",
    "  obs, reward, done, info = env.step(action)[:4]\n",
    "\n",
    "  previous_obs = obs\n",
    "  score += reward\n",
    "  step += 1\n",
    "\n",
    "  if done:\n",
    "    break\n",
    "\n",
    "print(score, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성공적인 에피소드 저장\n",
    "\n",
    "# 스텝당 -1, X좌표로 -0.2이상일 때 +1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "scores = []\n",
    "training_data = []\n",
    "accepted_scores = []\n",
    "\n",
    "required_score = -198\n",
    "\n",
    "for i in range(5000):\n",
    "  if i % 100 == 0:\n",
    "    print(i,end=' ')\n",
    "\n",
    "  env.reset()\n",
    "\n",
    "  score = 0\n",
    "\n",
    "  game_memory = []\n",
    "  previous_obs = []\n",
    " \n",
    "  for i in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)[:4]\n",
    "\n",
    "    if len(previous_obs) > 0:\n",
    "      game_memory.append([previous_obs, action])\n",
    "\n",
    "    previous_obs = obs\n",
    "\n",
    "    if obs[0] > -0.2:\n",
    "      reward = 1\n",
    "\n",
    "    else:\n",
    "      reward = -1\n",
    "\n",
    "    score += reward\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  scores.append(score)\n",
    "\n",
    "  if score > required_score:\n",
    "    accepted_scores.append(score)\n",
    "\n",
    "    for data in game_memory:\n",
    "      training_data.append(data)\n",
    "\n",
    "scores = np.array(scores)\n",
    "\n",
    "print(scores.mean())\n",
    "print(scores)\n",
    "print(accepted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array([i[0] for i in training_data]).reshape(-1, 2)\n",
    "train_Y = np.array([i[1] for i in training_data]).reshape(-1, 1)\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "\n",
    "print(train_X)\n",
    "print(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 Q 테이블 설정\n",
    "\n",
    "state_grid_count = 10\n",
    "action_grid_count = 6\n",
    "\n",
    "q_table = []\n",
    "\n",
    "for i in range(state_grid_count):\n",
    "  q_table.append([])\n",
    "\n",
    "  for j in range(state_grid_count):\n",
    "    q_table[i].append([])\n",
    "\n",
    "    for k in range(action_grid_count):\n",
    "      q_table[i][j].append(0.0001) #매우 작은 값으로 초기값 할당\n",
    "\n",
    "actions = range(action_grid_count)\n",
    "actions = np.array(actions).astype(float)\n",
    "\n",
    "actions *= ((env.action_space.high - env.action_space.low) / (action_grid_count - 1))\n",
    "actions += env.action_space.low\n",
    "\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def obs_to_state(env, obs):\n",
    "  obs = obs.flatten()\n",
    "  low = env.observation_space.low\n",
    "  high = env.observation_space.high\n",
    "\n",
    "  idx = (obs - low) / (high - low) * state_grid_count\n",
    "  idx = [int(x) for x in idx]\n",
    "\n",
    "  return idx\n",
    "\n",
    "def softmax(logits):\n",
    "  exp_logits = np.exp(logits - np.max(logits))\n",
    "  sum_exp_logits = np.sum(exp_logits)\n",
    "\n",
    "  return exp_logits / sum_exp_logits\n",
    "\n",
    "sample = env.observation_space.sample()\n",
    "grid = obs_to_state(env, sample)\n",
    "\n",
    "print(sample)\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수가 작을 때는 랜덤으로 행동하고 아니면 최적의 행동 선택\n",
    "# 처음에는 다양한 행동을 시도(탐험, exploration), 나중에는 최적의 행동(이용, exploitation)\n",
    "# 입실론은 학습 진행에 따라 점점 작아지도록 설정\n",
    "# 스텝마다 -0.05의 보상 증가, 에이전트가 움직이도록 자극하는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.experimental.wrappers import RecordVideoV0\n",
    "from IPython.display import Video\n",
    "\n",
    "max_episodes = 5\n",
    "\n",
    "scores = []\n",
    "steps = []\n",
    "select_actions = []\n",
    "\n",
    "learning_rate = 0.05\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\", render_mode='rgb_array')\n",
    "env = RecordVideoV0(env, \"video\", name_prefix=\"mount-q\", disable_logger=True,episode_trigger=lambda x: x % 2 == 0 )\n",
    "\n",
    "for i in range(max_episodes):\n",
    "    epsilon *= 0.9\n",
    "    epsilon = max(epsilon_min, epsilon)\n",
    "\n",
    "    previous_obs = env.reset()[0]\n",
    "\n",
    "    score = 0\n",
    "    step = 0\n",
    "    cnt=0\n",
    "\n",
    "    while True:\n",
    "        cnt+=1\n",
    "\n",
    "        if cnt%1000==0:\n",
    "            print(cnt,end=' ')\n",
    "\n",
    "        state_idx = obs_to_state(env, previous_obs)\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            action_idx = random.randint(0, action_grid_count-1)\n",
    "            action = actions[action_idx]\n",
    "        else:\n",
    "            logits = q_table[state_idx[0]][state_idx[1]]\n",
    "            action_idx = np.argmax(softmax(logits))\n",
    "            action = actions[action_idx]\n",
    "\n",
    "        obs, reward, done, info = env.step([action])[:4]\n",
    "\n",
    "        previous_obs = obs\n",
    "        score += reward\n",
    "        reward -= 0.05\n",
    "        step += 1\n",
    "\n",
    "        select_actions.append(action)\n",
    "        new_state_idx = obs_to_state(env, obs)\n",
    "        q_table[state_idx[0]][state_idx[1]][action_idx] = q_table[state_idx[0]][state_idx[1]][action_idx] + learning_rate * (reward + gamma * np.amax(q_table[new_state_idx[0]][new_state_idx[1]]) - q_table[state_idx[0]][state_idx[1]][action_idx])\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores.append(score)  \n",
    "    steps.append(step)\n",
    "\n",
    "    print('\\n', i, 'mean score: {}, mean step: {}, epsilon: {}'.format(np.mean(scores[-100:]), np.mean(steps[-100:]), epsilon))\n",
    "\n",
    "    if np.mean(scores[-100:]) >= 90:\n",
    "        print('Solved on episode {}!'.format(i))\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"video/mount-q-episode-4.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택된 행동의 비율\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.displot(select_actions)\n",
    "\n",
    "# 0 근처에 많이 발생 - 속도가 적을수록 적은 패널티의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('score')\n",
    "\n",
    "plt.show()\n",
    "# 점수가 증가하는 추세"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
